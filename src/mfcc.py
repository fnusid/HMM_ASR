'''
This code was generated by @Sidharth. This code calculates the mfcc of a given speech signal.

'''
import numpy as np
import librosa
import sounddevice as sd
import soundfile as sf
import os
import pdb
import matplotlib.pyplot as plt
import scipy
import librosa.display
from scipy.signal import get_window

class _WindowingMFCC:
    def __init__(self):
        pass

    def plot_spectrogram(self, specgram, title=None, ylabel="freq_bin", ax=None):
        if ax is None:
            _, ax = plt.subplots(1, 1)
        if title is not None:
            ax.set_title(title)
        ax.set_ylabel(ylabel)
        ax.imshow(librosa.power_to_db(specgram), origin="lower", aspect="auto", interpolation="nearest")

    # def _read_wav(self):
    #     duration = 2
    #     sr = 24000
    #     channels = 1
    #     if os.path.exists("/Users/sidharth/Desktop/speech_processing/audio/Replace.wav") == False:
    #         print("recording the audio")
    #         audio = sd.rec(int(sr * duration), samplerate=sr, channels=channels, dtype='float32')
    #         sd.wait()
    #         sf.write("/Users/sidharth/Desktop/speech_processing/audio/Replace.wav", audio, sr)
    #         print("recording saved at /Users/sidharth/Desktop/speech_processing/audio/Replace.wav")

    #     audio, sr = sf.read("/Users/sidharth/Desktop/speech_processing/audio/Replace.wav")
    #     return audio, sr

    def _windowing(self, audio, FFT_size=2048, hop_size=10, sr=24000):
        audio = np.pad(audio, int(FFT_size / 2), mode='reflect')
        frame_len = np.round(sr * hop_size / 1000).astype(int)
        frame_num = int((len(audio) - FFT_size) / frame_len) + 1
        frames = np.zeros((frame_num, FFT_size))

        for n in range(frame_num):
            frames[n] = audio[n * frame_len:n * frame_len + FFT_size]
        window = get_window("hann", FFT_size, fftbins=True)
        audio_window = frames * window
        audio_windowTrans = np.transpose(audio_window)

        audio_fft = np.empty((int(1 + 2048 // 2), audio_windowTrans.shape[1]), dtype=np.complex64, order='F')

        for n in range(audio_fft.shape[1]):
            audio_fft[:, n] = np.fft.fft(audio_windowTrans[:, n], axis=0)[:audio_fft.shape[0]]

        audio_fft = np.transpose(audio_fft)
        return np.array(audio_fft)

    def freq_to_mel(self, freq):
        return 2595.0 * np.log10(1.0 + freq / 700.0)

    def met_to_freq(self, mels):
        return 700.0 * (10.0 ** (mels / 2595.0) - 1.0)

    def filter_p(self, fmin, fmax, mel_filter_num, FFT_size=2048, sample_rate=24000):
        fmin_mel = self.freq_to_mel(fmin)
        fmax_mel = self.freq_to_mel(fmax)

        mels = np.linspace(fmin_mel, fmax_mel, num=mel_filter_num + 2)
        freqs = self.met_to_freq(mels)

        return np.floor((FFT_size + 1) / sample_rate * freqs).astype(int), freqs

    def dct_basis_functions(self, dct_filter_num=40, mel_filter_len=20):
        b_matrix = np.empty((dct_filter_num, mel_filter_len))
        b_matrix[0, :] = 1.0 / np.sqrt(mel_filter_len)

        samples = np.arange(1, 2 * mel_filter_len, 2) * np.pi / (2.0 * mel_filter_len)

        for i in range(1, dct_filter_num):
            b_matrix[i, :] = np.cos(i * samples) * np.sqrt(2.0 / mel_filter_len)

        return b_matrix

    def _mel_filter(self, mag_spec, sr):
        freq_min = 0
        freq_high = sr / 2
        mel_filter_num = 20
        filter_points, mel_freqs = self.filter_p(freq_min, freq_high, mel_filter_num)
        filters = np.zeros((len(filter_points) - 2, int(2048 / 2 + 1)))

        for n in range(len(filter_points) - 2):
            filters[n, filter_points[n]: filter_points[n + 1]] = np.linspace(0, 1, filter_points[n + 1] - filter_points[n])
            filters[n, filter_points[n + 1]: filter_points[n + 2]] = np.linspace(1, 0, filter_points[n + 2] - filter_points[n + 1])
        enormalization = 2.0 / (mel_freqs[2: mel_filter_num + 2] - mel_freqs[: mel_filter_num])
        filters *= enormalization[:, np.newaxis]
        mel_filters = filters
        mel_filter_wrapped_spec = mel_filters @ mag_spec.T
        mel_filter_wrapped_log_spec = 10 * np.log(mel_filter_wrapped_spec + 1e-9)

        return mel_filter_wrapped_log_spec

    def _plot_mfcc(self,mfcc, libr_mfcc_13):
        fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)
        img1 = librosa.display.specshow(mfcc, x_axis='time', ax=ax[0])
        ax[0].set(title='my MFCC')
        img2 = librosa.display.specshow(libr_mfcc_13, x_axis='time', ax=ax[1])
        ax[1].set(title='Librosa MFCC')
        plt.show()


    def run(self, audio, sr, plot_mfcc=False):
        audio, sr = audio, sr
        audio /= np.max(audio)  # peak normalizing
        windowed_speech_fft = self._windowing(audio=audio, sr=sr)
        audio_power = np.square(np.abs(windowed_speech_fft))
        mel_filter_wrapped_log_spec = self._mel_filter(mag_spec=audio_power, sr=sr)
        dct_speech = self.dct_basis_functions()
        mfcc_speech = np.dot(dct_speech, mel_filter_wrapped_log_spec)
        mfcc = mfcc_speech[:13, :]
        libr_mfcc = librosa.feature.mfcc(y=audio, sr=sr, dct_type=2, n_mfcc=128, n_fft=2048, win_length=int(0.025 * sr), hop_length=int(0.01 * sr))
        libr_mfcc_13 = libr_mfcc[:13, :]
        if plot_mfcc==True:
            self._plot_mfcc(mfcc=mfcc, libr_mfcc_13=libr_mfcc_13)
        return mfcc

if __name__ == '__main__':
    wmfcc = _WindowingMFCC()
    wmfcc.run(plot_mfcc=True)




